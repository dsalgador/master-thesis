{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import model\n",
    "import numpy as np\n",
    "import random\n",
    "import tank\n",
    "import truck\n",
    "\n",
    "import gym_pdsystem\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from matplotlib import animation, rc\n",
    "from IPython.display import HTML\n",
    "\n",
    "\n",
    "import time\n",
    "\n",
    "from gym_pdsystem.envs.pdsystem_env import PDSystemEnv\n",
    "import gym_pdsystem.utils.utilsq as ut\n",
    "import gym_pdsystem.utils.constants as ct\n",
    "import gym_pdsystem.utils.functions as fnc\n",
    "\n",
    "import os\n",
    "\n",
    "simulations_directory = './simulations'\n",
    "if not os.path.exists(simulations_directory):\n",
    "    os.makedirs(simulations_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "COEFF = ct.COEFF\n",
    "\n",
    "C_TRANSPORT = ct.C_TRANSPORT\n",
    "C_LEVELS = ct.C_LEVELS\n",
    "\n",
    "p0_GLOBAL = ct.p0_GLOBAL\n",
    "\n",
    "P1_GLOBAL = ct.P1_GLOBAL\n",
    "P2_GLOBAL = ct.P2_GLOBAL\n",
    "\n",
    "M_GLOBAL = ct.M_GLOBAL\n",
    "\n",
    "NOT_DELIVERYING_PENALTY = P2_GLOBAL #to be equivalent/same importance as having 0 stock or surpassing max capacity levels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System initializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "368640"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def initialize_test_system(seed = None):\n",
    "    if seed != None:\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    # Tanks' information\n",
    "    global n\n",
    "    n = 5 \n",
    "    tank_ids = list(range(1,n+1))\n",
    "    tank_max_loads =  np.array([100., 200, 100., 800., 200.])\n",
    "    C_max = np.array([ [load] for load in tank_max_loads ])\n",
    "\n",
    "    tank_current_loads = np.full(n,0)\n",
    "    tank_consumption_rates =  np.array([5.] * n)\n",
    "    noisy_consumption_rate = True\n",
    "    \n",
    "    global n_discrete_load_levels\n",
    "    n_discrete_load_levels = np.array([4,4,4,4,4])\n",
    "    \n",
    "    load_level_percentages = np.array([ #b , c, e\n",
    "                                            [0.02, 0.31, 0.9],\n",
    "                                            [0.01, 0.03, 0.9],\n",
    "                                            [0.05, 0.16, 0.9],\n",
    "                                            [0.07, 0.14, 0.85],\n",
    "                                            [0.08, 0.26, 0.9]\n",
    "                                               ])\n",
    "    for i in range(n):\n",
    "        tank_consumption_rates[i] = tank_max_loads[i] * (load_level_percentages[i][0] + load_level_percentages[i][1])/2.0\n",
    "        \n",
    "    tank_levels = np.multiply(C_max,load_level_percentages)\n",
    "    \n",
    "    for i, tank_level in enumerate(tank_levels):\n",
    "                a = tank_level[0]\n",
    "                b = tank_level[-1]\n",
    "                #current_load = 0.75 * (a+b)/2.0# np.random.randint(a+1,b, size =(1,)) GIVES A STRANGE ERROR\n",
    "                current_load = np.random.random() * (b - a-1) + a+1 #np.random.randint(a+1,b)\n",
    "                tank_current_loads[i] = current_load * 1.0\n",
    "                \n",
    "#     for i, (lvl, max_load) in enumerate(zip(n_discrete_load_levels, tank_max_loads)):\n",
    "#         a = np.linspace(0,max_load, lvl+1)[1]\n",
    "#         current_load = np.random.randint(a+1,max_load)\n",
    "#         tank_current_loads[i] = current_load     \n",
    "\n",
    "    # Trucks' information\n",
    "    global k\n",
    "    k = 2\n",
    "    truck_ids = list(range(k))\n",
    "    truck_max_loads = np.array([70.,130.])\n",
    "    truck_current_loads = truck_max_loads.copy()\n",
    "    truck_current_positions =  np.array([5] * k)\n",
    "    #truck_fractions_deliverable =  np.array([1.] * k) # we for now we only allow to deliver all the content of the truck\n",
    "    truck_fractions_deliverable =  np.array([ np.array([1.]), \n",
    "                                              np.array([1.])\n",
    "                                            ]) # we for now we only allow to deliver all the content of the truck\n",
    "    global n_discrete_load_levels_trucks\n",
    "    n_discrete_load_levels_trucks = np.array([1,1])\n",
    "\n",
    "    # System's information\n",
    "   \n",
    "    graph = ut.simple_graph(n+1)\n",
    "    tanks = [tank.Tank( tank_id, current_load, max_load, consumption_rate, n_lvls, d_lvls, noisy_consumption_rate) \n",
    "             for  tank_id, current_load, max_load, consumption_rate, n_lvls, d_lvls in \n",
    "             zip( tank_ids, tank_current_loads, tank_max_loads, tank_consumption_rates, n_discrete_load_levels,\n",
    "                  load_level_percentages)]\n",
    "    trucks = [truck.Truck( truck_id, current_load, max_load, current_position, load_fractions_deliverable, n_lvls) \n",
    "             for  truck_id, current_load, max_load, current_position, load_fractions_deliverable, n_lvls in \n",
    "             zip(truck_ids, truck_current_loads, truck_max_loads, truck_current_positions, \n",
    "                 truck_fractions_deliverable, n_discrete_load_levels_trucks)]\n",
    "\n",
    "    w =  np.array([32., 159., 162., 156.,156., 0.])\n",
    "\n",
    "    weights_matrix = ut.simple_weights(n+1, w)\n",
    "    \n",
    "    return(tanks, trucks, graph, weights_matrix)\n",
    "\n",
    "tanks, trucks, graph, weights_matrix = initialize_test_system()\n",
    "toy_system = model.System(tanks = tanks, trucks = trucks, adjacency_matrix = graph, weights_matrix = weights_matrix)\n",
    "\n",
    "# Action-State space dimension\n",
    "a_s_dim = toy_system.states_dim * toy_system.actions_dim\n",
    "a_s_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[35, 171, 67, 429, 42]\n",
      "[[0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [1 1 1 1 1 1]]\n",
      "[[ inf  inf  inf  inf  inf  inf]\n",
      " [ inf  inf  inf  inf  inf  inf]\n",
      " [ inf  inf  inf  inf  inf  inf]\n",
      " [ inf  inf  inf  inf  inf  inf]\n",
      " [ inf  inf  inf  inf  inf  inf]\n",
      " [ 32. 159. 162. 156. 156.   0.]]\n"
     ]
    }
   ],
   "source": [
    "tanks, trucks, graph, weights_matrix = initialize_test_system(seed = 42)\n",
    "system = model.System(tanks = tanks, trucks = trucks, adjacency_matrix = graph, weights_matrix = weights_matrix)\n",
    "print(system.tank_loads())\n",
    "print(system.graph)\n",
    "print(system.weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reinitialize system function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def reinitialize_system(system, seed = None):\n",
    "    if seed != None:\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "    for tank, tank_levels in zip(system.tanks, system.tanks_level):\n",
    "        a = tank_levels[0]\n",
    "        b = tank_levels[-1]\n",
    "        current_load = np.random.random() * (b - a-1) + a+1 #np.random.randint(a+1,b)*1.0\n",
    "        tank.load = current_load\n",
    "    system.reset_trucks_positions(); \n",
    "    return(system)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-learning algorithm (off-policy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEICAYAAABYoZ8gAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Retrain parameters:\n",
    "retrain = False\n",
    "\n",
    "episode_retrain = 2*10**3 # episode number from which start the retrain. Ensure that a file with this number in \n",
    "                          # the Q filename is available.\n",
    "retrain_episodes = 1*10**3\n",
    "\n",
    "\n",
    "# Train parameters:\n",
    "train_epsilon = True\n",
    "\n",
    "learning_rate0 = 1 \n",
    "learning_rate_decay = 0 \n",
    "\n",
    "episode_length = 30\n",
    "discount_rate = 0.9\n",
    "\n",
    "episodes = 3*10**5\n",
    "episodes_epsilon_min = 0.04*episodes\n",
    "\n",
    "# Output's frequencies\n",
    "train_freq = 10**2 # 10**4\n",
    "train_vis_freq =  10**3\n",
    "train_rew_freq =  10**2\n",
    "train_Q_freq =  10**3\n",
    "\n",
    "epsilon0 = 1.0\n",
    "epsilon_decay =( 1./(episodes_epsilon_min) ) \n",
    "epsilon_min = 0.05\n",
    "\n",
    "verbose = False\n",
    "verbose_info = False\n",
    "\n",
    "seed = 42\n",
    "\n",
    "simulation_id = 14\n",
    "\n",
    "# Visualization of the epsilon parameter value that will be present duing the simulation\n",
    "% matplotlib inline\n",
    "\n",
    "def epsilon_fnc(x, epsilon0 = epsilon0, decay = epsilon_decay, epsilon_min = epsilon_min):\n",
    "    e_mins = epsilon_min * np.ones(len(x))\n",
    "    eps = epsilon0 / (1.0 + (x-1)*epsilon_decay)\n",
    "    return np.maximum(e_mins, eps )\n",
    "\n",
    "t = np.arange(1, episodes, 1)\n",
    "epsilon = epsilon_fnc(t)\n",
    "\n",
    "plt.plot(t, epsilon)\n",
    "plt.axhline(y= epsilon_min, xmin=0, xmax=episodes, hold=None, color = \"Red\", linestyle = '--')\n",
    "\n",
    "plt.title(\"Epsilon parameter as a function of the number of episodes\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_visualization_steps = []\n",
    "train_rewards_list = []\n",
    "\n",
    "tanks, trucks, graph, weights_matrix = initialize_test_system()\n",
    "toy_system = model.System(tanks = tanks, trucks = trucks, adjacency_matrix = graph, weights_matrix = weights_matrix)\n",
    "\n",
    "Q = {}\n",
    "\n",
    "# Create directories for the simulations' outputs\n",
    "\n",
    "simulation_directory = './simulations/simulation{}'.format(simulation_id)\n",
    "if not os.path.exists(simulation_directory):\n",
    "    os.makedirs(simulation_directory)\n",
    "    os.makedirs(simulation_directory + '/Q-dictionaries')\n",
    "    os.makedirs(simulation_directory + '/discrewards')\n",
    "    os.makedirs(simulation_directory + '/vis')\n",
    "    \n",
    "else:\n",
    "    raise Exception(\"The simulation id you tried to use has been already used before. Try to change it to a new one.\")\n",
    "\n",
    "    \n",
    "ut.save_obj(toy_system, simulation_directory+\"/system-sim\"+f\"{simulation_id}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def episodic_train_Q_epsilon( \n",
    "            epsilon0 = epsilon0,\n",
    "            epsilon_min = epsilon_min,\n",
    "            n_episodes = episodes, \n",
    "            episode_length = episode_length,\n",
    "            learning_rate0 = learning_rate0,\n",
    "            learning_rate_decay = learning_rate_decay,\n",
    "            discount_rate = discount_rate,\n",
    "            system = toy_system,\n",
    "            Q = Q, verbose = verbose, verbose_info = verbose_info,\n",
    "            visualization_steps = train_visualization_steps, rewards_list = train_rewards_list,\n",
    "            seed = seed, \n",
    "            freq = train_freq,\n",
    "            vis_freq = train_vis_freq,\n",
    "            rew_freq = train_rew_freq,\n",
    "            Q_freq = train_Q_freq,\n",
    "            simulation_id = simulation_id,\n",
    "            round_time = 2\n",
    "    \n",
    "           ):\n",
    "    \n",
    "    time_start = time.time()\n",
    "    \n",
    "    for episode in range(1,n_episodes+1):\n",
    "        print(\"\\rEpisode: {}\".format(episode), end=\"\")\n",
    "\n",
    "        reinitialize_system(system, seed = episode)\n",
    "        \n",
    "        ### epsilon-greedy exploration\n",
    "        epsilon = max( epsilon_min, epsilon0 / (1+(episode-1)*epsilon_decay) ) \n",
    "        \n",
    "        ### decrement of learning rate\n",
    "        learning_rate = learning_rate0 / (1+(episode-1)*learning_rate_decay)        \n",
    "\n",
    "        discounted_reward = 0\n",
    "        \n",
    "        for t in range(episode_length):\n",
    "\n",
    "            system.update_state()\n",
    "            s_current = system.state_to_string()                \n",
    "            p = np.random.uniform()\n",
    "\n",
    "            if p > epsilon:\n",
    "                #DETERMINISTIC ACTION OPTIMAL\n",
    "                s0 = system.state_to_string()\n",
    "                best_action = optimal_policy(s0, Q)\n",
    "                if best_action == None:\n",
    "                    reward = system.random_action(seed = (seed + (t+1)*episode), verbose = verbose)\n",
    "                else:\n",
    "                    reward = system.deterministic_action(best_action)\n",
    "                #print(best_action)\n",
    "            else:\n",
    "                reward = system.random_action(seed = (seed + (t+1)*episode), verbose = verbose)\n",
    "\n",
    "            a_current = system.action_to_string()\n",
    "            sa_current = ''.join([s_current, a_current])\n",
    "\n",
    "            system.update_state()\n",
    "            sa_new = system.state_action_to_string()\n",
    "\n",
    "            if ut.is_key(Q, sa_current) == False:\n",
    "                Q[sa_current] = 0\n",
    "\n",
    "            Q_max = max([Q[key] for key in Q.keys() if key.startswith(sa_new[0:system.state_length])]+[0.0]) \n",
    "\n",
    "            if Q[sa_current] != -np.inf:\n",
    "                Q[sa_current] = ( (1-learning_rate) * Q[sa_current] \n",
    "                                 + learning_rate* (reward + discount_rate * Q_max)\n",
    "                                )\n",
    "                \n",
    "            discounted_reward = discounted_reward + (discount_rate**t) * reward\n",
    "            system.reset_trucks_positions();     \n",
    "            system.reset_trucks_loads();\n",
    "            \n",
    "        #rewards_list.append(discounted_reward);\n",
    "        if episode % freq == 0:\n",
    "                time_end = time.time()\n",
    "                print(\". Elapsed time \", round( (time_end-time_start)/60., round_time), \" minuts.\",\n",
    "                      \"epsilon\", round(epsilon,4), \n",
    "                     \"Discounted reward: \", discounted_reward)\n",
    "                \n",
    "                if verbose_info:\n",
    "                    print(\"s, a\", system.s, system.a)\n",
    "                    print(\"ds, da\", system.ds, system.da)\n",
    "        if episode % Q_freq == 0:           \n",
    "                ut.save_obj(Q, simulation_directory +\"/Q-dictionaries/Q-dict-sim\" + f\"{simulation_id}\" + \"-\" + f\"{episode}\")   \n",
    "\n",
    "        if episode % vis_freq == 0:\n",
    "                #Save visualization and rewards\n",
    "                visualization_steps.append(toy_system.visualize());\n",
    "                ut.save_obj(visualization_steps, simulation_directory +\"/vis/vis-train-sim\" + f\"{simulation_id}\" + \"-\" + f\"{episode}\")   \n",
    "                                \n",
    "        if episode % rew_freq == 0:\n",
    "                rewards_list.append(discounted_reward);\n",
    "                ut.save_obj(rewards_list, simulation_directory +\"/discrewards/discrew-train-sim\" + f\"{simulation_id}\" + \"-\" + f\"{episode}\")\n",
    "                \n",
    "    end_time = round(time.time()-time_start,round_time)        \n",
    "    print(f\"Training finished. Total episodes: {n_episodes}. Elapsed time: {round(end_time/60., round_time)} minutes.\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Given a state, returns the action that has the highest Q-value.\n",
    "\n",
    "def optimal_policy(state, Q, system = toy_system):\n",
    "    \"\"\"\n",
    "    state must be in the string-integers code\n",
    "    \"\"\"\n",
    "    state_keys = [key for key in list(Q) if key.startswith(state)]\n",
    "    if len(state_keys) == 0:\n",
    "        return(None)\n",
    "    \n",
    "    state_q = [Q[state_key] for state_key in state_keys]\n",
    "    \n",
    "    #print(\"state_q \", state_q[1:min(10,len(state_q))])\n",
    "    \n",
    "    max_q = max(state_q)\n",
    "    #print(\"max_q\", max_q)\n",
    "    optimal_key_index = np.where(np.isin(state_q, max_q ))[0][0]\n",
    "    #print(\"optimal_key_index\", optimal_key_index)\n",
    "    optimal_key = state_keys[optimal_key_index]\n",
    "    #print(\"optimal_key\", optimal_key)\n",
    "    optimal_action = optimal_key[system.state_length:]\n",
    "    \n",
    "    return(optimal_action)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 100. Elapsed time  0.05  minuts. epsilon 0.9918 Discounted reward:  -8662111.607445398\n",
      "Episode: 200. Elapsed time  0.12  minuts. epsilon 0.9837 Discounted reward:  -8597568.312087744\n",
      "Episode: 300. Elapsed time  0.19  minuts. epsilon 0.9757 Discounted reward:  -7241482.211825967\n",
      "Episode: 400. Elapsed time  0.27  minuts. epsilon 0.9678 Discounted reward:  -7580185.079477649\n",
      "Episode: 500. Elapsed time  0.37  minuts. epsilon 0.9601 Discounted reward:  -8842102.192341417\n",
      "Episode: 560"
     ]
    }
   ],
   "source": [
    "if train_epsilon == True and retrain == False:\n",
    "    episodic_train_Q_epsilon()\n",
    "elif train_epsilon == False and retrain == True:\n",
    "    Q_retrain = ut.load_obj(simulation_directory + \"/Q-dictionaries/Q-dict-sim\" + f\"{simulation_id}\" + \"-\" + f\"{episode_retrain}\")\n",
    "    episodic_train_Q_epsilon(n_episodes = retrain_episodes, Q = Q_retrain)\n",
    "else:\n",
    "    raise Exception(\"Only one of the parameters train_epsilon or retrain parameters can be set to True.\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Episode: 100. Elapsed time  0.06  minuts. epsilon 0.9918 Discounted reward:  -8662111.607445398\n",
    "Episode: 200. Elapsed time  0.12  minuts. epsilon 0.9837 Discounted reward:  -8597568.312087744\n",
    "Episode: 300. Elapsed time  0.21  minuts. epsilon 0.9757 Discounted reward:  -7241482.211825967\n",
    "Episode: 400. Elapsed time  0.28  minuts. epsilon 0.9678 Discounted reward:  -7580185.079477649\n",
    "Episode: 500. Elapsed time  0.37  minuts. epsilon 0.9601 Discounted reward:  -8842102.192341417\n",
    "Episode: 600. Elapsed time  0.46  minuts. epsilon 0.9525 Discounted reward:  -8211861.012427437\n",
    "Episode: 700. Elapsed time  0.56  minuts. epsilon 0.945 Discounted reward:  -8896062.817336274\n",
    "Episode: 800. Elapsed time  0.66  minuts. epsilon 0.9376 Discounted reward:  -6615911.6603824645\n",
    "Episode: 900. Elapsed time  0.78  minuts. epsilon 0.9303 Discounted reward:  -6233749.3853852\n",
    "Episode: 1000. Elapsed time  0.91  minuts. epsilon 0.9231 Discounted reward:  -8736348.089337606\n",
    "Episode: 1100. Elapsed time  1.03  minuts. epsilon 0.9161 Discounted reward:  -9346062.015628532\n",
    "Episode: 1200. Elapsed time  1.16  minuts. epsilon 0.9092 Discounted reward:  -9070611.930038633\n",
    "Episode: 1300. Elapsed time  1.29  minuts. epsilon 0.9023 Discounted reward:  -9456665.421162536\n",
    "Episode: 1400. Elapsed time  1.43  minuts. epsilon 0.8956 Discounted reward:  -9170824.778201863\n",
    "Episode: 1441\n",
    "\n",
    "â€‹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST PARAMETERS AND INITIALIZATION\n",
    "\n",
    "# Initialize system\n",
    "episodes = 243000\n",
    "\n",
    "tanks, trucks, graph, weights_matrix = initialize_test_system(seed =episodes+1)\n",
    "test_toy_system = model.System(tanks = tanks, trucks = trucks, adjacency_matrix = graph, weights_matrix = weights_matrix)\n",
    "\n",
    "Q = ut.load_obj(simulation_directory + \"/Q-dictionaries/Q-dict-sim\" + f\"{simulation_id}\" + \"-\" + f\"{episodes}\")\n",
    "\n",
    "\n",
    "test_episodes = 10\n",
    "episode_length =30\n",
    "test_freq = 1\n",
    "test_verbose = False\n",
    "\n",
    "test_visualization_steps = []\n",
    "test_rewards_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_Q(n_episodes = test_episodes, \n",
    "           episode_length = episode_length,\n",
    "           system = test_toy_system,\n",
    "           visualization_steps = test_visualization_steps, \n",
    "           rewards_list = test_rewards_list,\n",
    "           freq = test_freq,\n",
    "           test_verbose = test_verbose\n",
    "           \n",
    "          ):\n",
    "    \n",
    "    for episode in range(1,n_episodes+1): \n",
    "        print(\"\\rTest episode: {}\".format(episode), end=\"\")\n",
    "        reinitialize_system(system, seed = episode+episodes)\n",
    "\n",
    "        discounted_reward = 0      \n",
    "        \n",
    "        for i in range(1,episode_length+1):\n",
    "            #print(\"state\", test_toy_system.s, test_toy_system.ds)\n",
    "            system.update_state()\n",
    "\n",
    "            #Save visualization steps\n",
    "            if i % freq == 0:\n",
    "                visualization_steps.append(system.visualize());\n",
    "\n",
    "            s0 = system.state_to_string()\n",
    "            best_action = optimal_policy(s0, Q)\n",
    "            #print(\"best_action\", best_action)\n",
    "\n",
    "            if best_action == None:\n",
    "                reward = system.random_action()\n",
    "                if i % freq == 0:\n",
    "                    if test_verbose == True:\n",
    "                        print(\"Episode\", episode, \"t\", i-1, reward, \" Random action is performed. Current state unknown for Q.\")\n",
    "\n",
    "            else:\n",
    "                reward = system.deterministic_action(best_action)\n",
    "                if i % freq == 0:\n",
    "                    if test_verbose == True:\n",
    "                        print(\"Episode\", episode, \"t\",i-1,reward, best_action)\n",
    "\n",
    "            system.reset_trucks_positions();\n",
    "            system.reset_trucks_loads();\n",
    "            \n",
    "            discounted_reward = discounted_reward + (discount_rate**(i-1)) * reward\n",
    "            if reward <= P2_GLOBAL:\n",
    "                print(\"\\rSome tank is in a forbidden level\")\n",
    "\n",
    "        system.reset_trucks_positions();\n",
    "        \n",
    "        #Save rewards\n",
    "        if episode % freq == 0:\n",
    "            rewards_list.append(discounted_reward);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_Q()\n",
    "#print(np.mean(test_rewards_list) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "mpl.rcParams.update(mpl.rcParamsDefault)\n",
    "plt.hist(test_rewards_list, bins = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualizing test simulation:\n",
    "\n",
    "test_anim = ut.create_system_animation(test_visualization_steps, test_episodes * episode_length,test_freq)\n",
    "HTML(test_anim.to_html5_video())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualizing train simulation:\n",
    "\n",
    "episode =episodes\n",
    "#simulation_id = 2\n",
    "step = 30\n",
    "discrewards_list = ut.load_obj(simulation_directory+\"/discrewards/discrew-train-sim\" + f\"{simulation_id}\" + \"-\" + f\"{episode}\")\n",
    "\n",
    "discrewards_list2 = [discrewards_list[i] for i in range(0,len(discrewards_list),step)]\n",
    "\n",
    "p = plt.plot([i for i in range(0,len(discrewards_list),step)], \n",
    "             discrewards_list2)\n",
    "\n",
    "#plt.show()\n",
    "plt.savefig(simulation_directory+'/discrewards-sim' + f'{simulation_id}' + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(Q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tank_levels = [test_visualization_steps[i][2] for i in range(len(test_visualization_steps))]\n",
    "tank_levels_array = np.asarray(tank_levels).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DISCRETE LEVELS\n",
    "\n",
    "cmap = plt.get_cmap('gnuplot')\n",
    "colors = [cmap(i) for i in np.linspace(0, 1, n)]\n",
    "lvl_colors = [\"Orange\", \"Green\",\"Orange\"]\n",
    "\n",
    "tanks = test_toy_system.tanks\n",
    "\n",
    "for i, color in enumerate(colors, start=1):\n",
    "    plt.subplot(3,3, i)    \n",
    "    plt.plot(tank_levels_array[i-1], color=color, label='Tank ${i}$'.format(i=i))\n",
    "    \n",
    "    plt.axhline(y= tanks[i-1].max_load, xmin=0, xmax=episode_length, hold=None, color = \"Red\", linestyle = '--')\n",
    "    for lvl_color, lvl in zip(lvl_colors, tanks[i-1].levels):\n",
    "        plt.axhline(y= lvl, xmin=0, xmax=episode_length, hold=None, color = lvl_color, linestyle = '--')\n",
    "    plt.axhline(y= 0, xmin=0, xmax=episode_length, hold=None, color = \"Red\", linestyle = '--')\n",
    "\n",
    "\n",
    "    plt.legend(loc='best')\n",
    "plt.show()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THE REAL LEVELS (percentages 12h, 36h, ? h)\n",
    "\n",
    "cmap = plt.get_cmap('gnuplot')\n",
    "colors = [cmap(i) for i in np.linspace(0, 1, n)]\n",
    "lvl_colors = [\"Orange\", \"Green\",\"Orange\"]\n",
    "\n",
    "tanks = test_toy_system.tanks\n",
    "\n",
    "for i, color in enumerate(colors, start=1):\n",
    "    plt.subplot(3,3, i)    \n",
    "\n",
    "    plt.plot(tank_levels_array[i-1], color=color, label='Tank ${i}$'.format(i=i))\n",
    "    \n",
    "    plt.axhline(y= tanks[i-1].max_load, xmin=0, xmax=episode_length, hold=None, color = \"Red\", linestyle = '--')\n",
    "    for lvl_color, lvl in zip(lvl_colors, tanks[i-1].level_percentages):\n",
    "        plt.axhline(y= lvl * tanks[i-1].max_load, xmin=0, xmax=episode_length, hold=None, color = lvl_color, \n",
    "                    linestyle = '--')\n",
    "    plt.axhline(y= 0, xmin=0, xmax=episode_length, hold=None, color = \"Red\", linestyle = '--')\n",
    "    \n",
    "    \n",
    "    percentages = tanks[i-1].level_percentages           \n",
    "    c = percentages[1]\n",
    "    e = percentages[2]          \n",
    "    d = ct.p0_GLOBAL*e+(1-ct.p0_GLOBAL)*c\n",
    "    plt.axhline(y= d*tanks[i-1].max_load, xmin=0, xmax=episode_length, hold=None, color = \"lawngreen\", \n",
    "                linestyle = '-.')\n",
    "\n",
    "    plt.axhline(y= np.mean(tank_levels_array[i-1]), xmin=0, xmax=episode_length, hold=None, \n",
    "                color = \"blue\", linestyle = '-.')\n",
    "  \n",
    "\n",
    "\n",
    "    plt.legend(loc='best')\n",
    "#plt.show()\n",
    "plt.savefig(simulation_directory + '/tank-levels-sim' + f'{simulation_id}' + '.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Episode: 100. Elapsed time  0.05  minuts. epsilon 0.9579 Discounted reward:  -28000041.108963497\n",
    "Episode: 200. Elapsed time  0.11  minuts. epsilon 0.9187 Discounted reward:  -26000043.136069976\n",
    "Episode: 300. Elapsed time  0.18  minuts. epsilon 0.8827 Discounted reward:  -20000040.21466474\n",
    "Episode: 400. Elapsed time  0.27  minuts. epsilon 0.8494 Discounted reward:  -25000023.10632617\n",
    "Episode: 500. Elapsed time  0.37  minuts. epsilon 0.8185 Discounted reward:  -24000028.114429202\n",
    "Episode: 600. Elapsed time  0.5  minuts. epsilon 0.7898 Discounted reward:  -26000033.119467866\n",
    "Episode: 700. Elapsed time  0.63  minuts. epsilon 0.763 Discounted reward:  -24000030.426938158\n",
    "Episode: 800. Elapsed time  0.76  minuts. epsilon 0.7379 Discounted reward:  -22000037.105293885\n",
    "Episode: 900. Elapsed time  0.88  minuts. epsilon 0.7145 Discounted reward:  -16000027.214738708\n",
    "Episode: 1000. Elapsed time  1.01  minuts. epsilon 0.6925 Discounted reward:  -13000021.99472654\n",
    "Episode: 1100. Elapsed time  1.16  minuts. epsilon 0.6718 Discounted reward:  -15000024.008988662\n",
    "Episode: 1200. Elapsed time  1.31  minuts. epsilon 0.6524 Discounted reward:  -18000029.226717632\n",
    "Episode: 1300. Elapsed time  1.47  minuts. epsilon 0.634 Discounted reward:  -19000028.52769577\n",
    "Episode: 1400. Elapsed time  1.63  minuts. epsilon 0.6166 Discounted reward:  -22000036.44480863\n",
    "Episode: 1500. Elapsed time  1.8  minuts. epsilon 0.6002 Discounted reward:  -12000018.0040637\n",
    "Episode: 1600. Elapsed time  1.98  minuts. epsilon 0.5846 Discounted reward:  -16000031.228442688\n",
    "Episode: 1700. Elapsed time  2.16  minuts. epsilon 0.5698 Discounted reward:  -23000051.112380963\n",
    "Episode: 1800. Elapsed time  2.37  minuts. epsilon 0.5557 Discounted reward:  -13000020.038069973\n",
    "Episode: 1900. Elapsed time  2.63  minuts. epsilon 0.5423 Discounted reward:  -21000021.34935867\n",
    "Episode: 2000. Elapsed time  2.84  minuts. epsilon 0.5295 Discounted reward:  -12000014.993348014\n",
    "Episode: 2100. Elapsed time  3.04  minuts. epsilon 0.5174 Discounted reward:  -13000017.050656565\n",
    "Episode: 2200. Elapsed time  3.24  minuts. epsilon 0.5057 Discounted reward:  -18000029.338394843\n",
    "Episode: 2300. Elapsed time  3.45  minuts. epsilon 0.4946 Discounted reward:  -9000007.996369414\n",
    "Episode: 2400. Elapsed time  3.65  minuts. epsilon 0.484 Discounted reward:  -20000028.995834745\n",
    "Episode: 2500. Elapsed time  3.87  minuts. epsilon 0.4738 Discounted reward:  -12000008.99960373\n",
    "Episode: 2600. Elapsed time  4.09  minuts. epsilon 0.464 Discounted reward:  -8000001.990621154\n",
    "Episode: 2700. Elapsed time  4.32  minuts. epsilon 0.4546 Discounted reward:  -15000022.313438928\n",
    "Episode: 2800. Elapsed time  4.55  minuts. epsilon 0.4456 Discounted reward:  -11000011.050444487\n",
    "Episode: 2900. Elapsed time  4.79  minuts. epsilon 0.437 Discounted reward:  -7000012.998633672\n",
    "Episode: 3000. Elapsed time  5.02  minuts. epsilon 0.4287 Discounted reward:  -8000005.264682835\n",
    "Episode: 3100. Elapsed time  5.27  minuts. epsilon 0.4206 Discounted reward:  -11000018.992927592\n",
    "Episode: 3200. Elapsed time  5.5  minuts. epsilon 0.4129 Discounted reward:  -7000009.997351216\n",
    "Episode: 3300. Elapsed time  5.77  minuts. epsilon 0.4055 Discounted reward:  -13000011.030119121\n",
    "Episode: 3400. Elapsed time  6.0  minuts. epsilon 0.3983 Discounted reward:  -18000023.3058256\n",
    "Episode: 3500. Elapsed time  6.24  minuts. epsilon 0.3914 Discounted reward:  -7000002.996427942\n",
    "Episode: 3600. Elapsed time  6.49  minuts. epsilon 0.3847 Discounted reward:  -11000007.309740528\n",
    "Episode: 3700. Elapsed time  6.75  minuts. epsilon 0.3782 Discounted reward:  -11000012.001738582\n",
    "Episode: 3800. Elapsed time  7.01  minuts. epsilon 0.372 Discounted reward:  -8000003.996528433\n",
    "Episode: 3900. Elapsed time  7.31  minuts. epsilon 0.3659 Discounted reward:  -9000012.050633566\n",
    "Episode: 4000. Elapsed time  7.58  minuts. epsilon 0.3601 Discounted reward:  -6000005.9967362685\n",
    "Episode: 4100. Elapsed time  7.85  minuts. epsilon 0.3544 Discounted reward:  -7000008.991066201\n",
    "Episode: 4200. Elapsed time  8.11  minuts. epsilon 0.3489 Discounted reward:  -8000002.004340031\n",
    "Episode: 4300. Elapsed time  8.39  minuts. epsilon 0.3436 Discounted reward:  -6000007.205459152\n",
    "Episode: 4400. Elapsed time  8.66  minuts. epsilon 0.3384 Discounted reward:  -7000008.088827883\n",
    "Episode: 4500. Elapsed time  8.92  minuts. epsilon 0.3334 Discounted reward:  -10000008.016246885\n",
    "Episode: 4600. Elapsed time  9.21  minuts. epsilon 0.3285 Discounted reward:  -5000008.212606534\n",
    "Episode: 4700. Elapsed time  9.51  minuts. epsilon 0.3238 Discounted reward:  -8000007.021665757\n",
    "Episode: 4800. Elapsed time  9.79  minuts. epsilon 0.3192 Discounted reward:  -8000008.038475299\n",
    "Episode: 4900. Elapsed time  10.07  minuts. epsilon 0.3147 Discounted reward:  -18000020.03056307\n",
    "Episode: 5000. Elapsed time  10.39  minuts. epsilon 0.3104 Discounted reward:  -10000006.999470046\n",
    "Episode: 5100. Elapsed time  10.76  minuts. epsilon 0.3062 Discounted reward:  -7000004.998659743\n",
    "Episode: 5200. Elapsed time  11.05  minuts. epsilon 0.3021 Discounted reward:  -7000007.06712629\n",
    "Episode: 5300. Elapsed time  11.38  minuts. epsilon 0.2981 Discounted reward:  -8000009.999546508\n",
    "Episode: 5400. Elapsed time  11.69  minuts. epsilon 0.2942 Discounted reward:  -5000000.995678231\n",
    "Episode: 5500. Elapsed time  11.99  minuts. epsilon 0.2904 Discounted reward:  -4000003.9933600356\n",
    "Episode: 5600. Elapsed time  12.29  minuts. epsilon 0.2867 Discounted reward:  -11000004.996177264\n",
    "Episode: 5700. Elapsed time  12.59  minuts. epsilon 0.2831 Discounted reward:  -6000003.999741462\n",
    "Episode: 5800. Elapsed time  12.9  minuts. epsilon 0.2795 Discounted reward:  -2000006.9925141241\n",
    "Episode: 5900. Elapsed time  13.2  minuts. epsilon 0.2761 Discounted reward:  -8000007.987305785\n",
    "Episode: 6000. Elapsed time  13.54  minuts. epsilon 0.2728 Discounted reward:  -4000000.993550161"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
